# =============================================================================
# Chive Production Docker Compose
# =============================================================================
# Single-server deployment for Tier 1 ($20-50/month VPS)
# Runs entire stack: API, Indexer, Frontend, and all databases
#
# Usage:
#   docker compose -f docker/docker-compose.prod.yml up -d
#
# Prerequisites:
#   1. Copy .env.production.example to .env.production and configure
#   2. Create required directories: mkdir -p data/{postgres,redis,elasticsearch,neo4j}
#   3. Build the application: docker build -t chive:latest -f docker/Dockerfile .
#
# Estimated resources: 10GB RAM minimum, 16GB recommended
# =============================================================================

services:
  # ---------------------------------------------------------------------------
  # Reverse Proxy - Traefik (auto-SSL with Let's Encrypt)
  # ---------------------------------------------------------------------------
  traefik:
    image: traefik:latest
    container_name: chive-traefik
    restart: unless-stopped
    command:
      - "--api.dashboard=true"
      - "--providers.docker=true"
      - "--providers.docker.exposedbydefault=false"
      - "--entrypoints.web.address=:80"
      - "--entrypoints.websecure.address=:443"
      - "--entrypoints.web.http.redirections.entrypoint.to=websecure"
      - "--entrypoints.web.http.redirections.entrypoint.scheme=https"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge=true"
      - "--certificatesresolvers.letsencrypt.acme.httpchallenge.entrypoint=web"
      - "--certificatesresolvers.letsencrypt.acme.email=${ACME_EMAIL}"
      - "--certificatesresolvers.letsencrypt.acme.storage=/letsencrypt/acme.json"
    ports:
      - "80:80"
      - "443:443"
    volumes:
      - /var/run/docker.sock:/var/run/docker.sock:ro
      - traefik_certs:/letsencrypt
    networks:
      - chive-network
    # Dashboard disabled externally - access via SSH tunnel to localhost:8080 if needed
    # labels:
    #   - "traefik.enable=true"
    #   - "traefik.http.routers.traefik.rule=Host(`traefik.${DOMAIN}`)"
    #   - "traefik.http.routers.traefik.entrypoints=websecure"
    #   - "traefik.http.routers.traefik.service=api@internal"
    #   - "traefik.http.routers.traefik.tls.certresolver=letsencrypt"
    #   - "traefik.http.routers.traefik.middlewares=traefik-auth"
    #   - "traefik.http.middlewares.traefik-auth.basicauth.users=${TRAEFIK_BASIC_AUTH}"

  # ---------------------------------------------------------------------------
  # Chive Web Frontend (Next.js)
  # ---------------------------------------------------------------------------
  chive-web:
    build:
      context: ..
      dockerfile: web/Dockerfile
      args:
        NEXT_PUBLIC_API_URL: https://${DOMAIN}/api
        NEXT_PUBLIC_FARO_URL: https://faro.${DOMAIN}/collect
        NEXT_PUBLIC_APP_VERSION: ${CHIVE_VERSION:-latest}
        NEXT_PUBLIC_FARO_TRACE_SAMPLE_RATE: "0.1"
        NEXT_PUBLIC_FARO_SESSION_SAMPLE_RATE: "0.5"
    image: chive-web:${CHIVE_VERSION:-latest}
    container_name: chive-web
    restart: unless-stopped
    environment:
      - NODE_ENV=production
    depends_on:
      chive-api:
        condition: service_healthy
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 512M
        reservations:
          cpus: '0.25'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:3001/"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.web.rule=Host(`${DOMAIN}`)"
      - "traefik.http.routers.web.entrypoints=websecure"
      - "traefik.http.routers.web.tls.certresolver=letsencrypt"
      - "traefik.http.services.web.loadbalancer.server.port=3001"

  # ---------------------------------------------------------------------------
  # Governance PDS (Bluesky PDS for community governance records)
  # ---------------------------------------------------------------------------
  governance-pds:
    image: ghcr.io/bluesky-social/pds:latest
    container_name: chive-governance-pds
    restart: unless-stopped
    environment:
      PDS_HOSTNAME: governance.${DOMAIN}
      PDS_DATA_DIRECTORY: /pds
      PDS_BLOBSTORE_DISK_LOCATION: /pds/blocks
      PDS_DID_PLC_URL: https://plc.directory
      PDS_BSKY_APP_VIEW_URL: https://api.bsky.app
      PDS_BSKY_APP_VIEW_DID: did:web:api.bsky.app
      PDS_REPORT_SERVICE_URL: https://mod.bsky.app
      PDS_REPORT_SERVICE_DID: did:plc:ar7c4by46qjdydhdevvrndac
      PDS_CRAWLERS: https://bsky.network
      PDS_JWT_SECRET: ${GOVERNANCE_PDS_JWT_SECRET:?GOVERNANCE_PDS_JWT_SECRET is required}
      PDS_ADMIN_PASSWORD: ${GOVERNANCE_PDS_ADMIN_PASSWORD:?GOVERNANCE_PDS_ADMIN_PASSWORD is required}
      PDS_PLC_ROTATION_KEY_K256_PRIVATE_KEY_HEX: ${GOVERNANCE_PDS_ROTATION_KEY:?GOVERNANCE_PDS_ROTATION_KEY is required}
    volumes:
      - governance_pds_data:/pds
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-q", "--spider", "http://127.0.0.1:3000/xrpc/_health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.governance-pds.rule=Host(`governance.${DOMAIN}`)"
      - "traefik.http.routers.governance-pds.entrypoints=websecure"
      - "traefik.http.routers.governance-pds.tls.certresolver=letsencrypt"
      - "traefik.http.services.governance-pds.loadbalancer.server.port=3000"

  # ---------------------------------------------------------------------------
  # Chive Documentation (Docusaurus)
  # ---------------------------------------------------------------------------
  # NOTE: Docs are deployed separately via deploy-docs.yml workflow using
  # docker-compose.docs.yml (nginx serving static files). This service is
  # included in the 'docs' profile for standalone testing only.
  chive-docs:
    build:
      context: ..
      dockerfile: docs/Dockerfile
    image: chive-docs:${CHIVE_VERSION:-latest}
    container_name: chive-docs
    restart: unless-stopped
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.25'
          memory: 128M
        reservations:
          cpus: '0.1'
          memory: 64M
    healthcheck:
      test: ["CMD-SHELL", "wget -q -S -O /dev/null http://127.0.0.1:80/ 2>&1 | grep -q 'HTTP/'"]
      interval: 30s
      timeout: 5s
      retries: 3
      start_period: 10s
    profiles:
      - docs
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.docs.rule=Host(`docs.${DOMAIN}`)"
      - "traefik.http.routers.docs.entrypoints=websecure"
      - "traefik.http.routers.docs.tls.certresolver=letsencrypt"
      - "traefik.http.services.docs.loadbalancer.server.port=80"

  # ---------------------------------------------------------------------------
  # Database Migrations (runs once before API starts)
  # ---------------------------------------------------------------------------
  chive-migrate:
    image: chive:${CHIVE_VERSION:-latest}
    container_name: chive-migrate
    command: ["sh", "-c", "node node_modules/node-pg-migrate/bin/node-pg-migrate up --migrations-dir dist/src/storage/postgresql/migrations --ignore-pattern '.*\\.(map|d\\.ts)$'"]
    env_file:
      - .env.production
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-chive}:${POSTGRES_PASSWORD}@postgres:5432/${POSTGRES_DB:-chive}
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - chive-network
    restart: "no"

  # ---------------------------------------------------------------------------
  # Elasticsearch Setup (runs once before API starts)
  # ---------------------------------------------------------------------------
  chive-es-setup:
    image: chive:${CHIVE_VERSION:-latest}
    container_name: chive-es-setup
    command: ["node", "--enable-source-maps", "dist/scripts/db/setup-elasticsearch.js"]
    env_file:
      - .env.production
    depends_on:
      elasticsearch:
        condition: service_healthy
    networks:
      - chive-network
    restart: "no"

  # ---------------------------------------------------------------------------
  # Neo4j Setup (runs once before API starts)
  # ---------------------------------------------------------------------------
  chive-neo4j-setup:
    image: chive:${CHIVE_VERSION:-latest}
    container_name: chive-neo4j-setup
    command: ["node", "--enable-source-maps", "dist/scripts/db/setup-neo4j.js"]
    env_file:
      - .env.production
    depends_on:
      neo4j:
        condition: service_healthy
    networks:
      - chive-network
    restart: "no"

  # ---------------------------------------------------------------------------
  # Chive API (AppView)
  # ---------------------------------------------------------------------------
  chive-api:
    image: chive:${CHIVE_VERSION:-latest}
    container_name: chive-api
    restart: unless-stopped
    env_file:
      - .env.production
    environment:
      - NODE_ENV=production
      - PORT=3000
      - OTEL_SERVICE_NAME=chive-appview
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://alloy:4318
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      neo4j:
        condition: service_healthy
      chive-migrate:
        condition: service_completed_successfully
      chive-es-setup:
        condition: service_completed_successfully
      chive-neo4j-setup:
        condition: service_completed_successfully
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:3000/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    labels:
      - "traefik.enable=true"
      # API subdomain route
      - "traefik.http.routers.api.rule=Host(`api.${DOMAIN}`)"
      - "traefik.http.routers.api.entrypoints=websecure"
      - "traefik.http.routers.api.tls.certresolver=letsencrypt"
      - "traefik.http.services.api.loadbalancer.server.port=3000"
      # Path-based route for /api on main domain (used by Next.js frontend)
      - "traefik.http.routers.api-path.rule=Host(`${DOMAIN}`) && PathPrefix(`/api`)"
      - "traefik.http.routers.api-path.entrypoints=websecure"
      - "traefik.http.routers.api-path.tls.certresolver=letsencrypt"
      - "traefik.http.routers.api-path.service=api"
      - "traefik.http.middlewares.api-strip.stripprefix.prefixes=/api"
      - "traefik.http.routers.api-path.middlewares=api-strip,api-ratelimit"
      # Rate limiting middleware
      - "traefik.http.middlewares.api-ratelimit.ratelimit.average=100"
      - "traefik.http.middlewares.api-ratelimit.ratelimit.burst=50"
      - "traefik.http.routers.api.middlewares=api-ratelimit"

  # ---------------------------------------------------------------------------
  # Chive Indexer (Firehose Consumer)
  # ---------------------------------------------------------------------------
  chive-indexer:
    image: chive:${CHIVE_VERSION:-latest}
    container_name: chive-indexer
    restart: unless-stopped
    command: ["node", "--enable-source-maps", "dist/src/indexer.js"]
    env_file:
      - .env.production
    environment:
      - NODE_ENV=production
      - OTEL_SERVICE_NAME=chive-indexer
      - OTEL_EXPORTER_OTLP_ENDPOINT=http://alloy:4318
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_healthy
      elasticsearch:
        condition: service_healthy
      chive-migrate:
        condition: service_completed_successfully
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '1.0'
          memory: 1G
        reservations:
          cpus: '0.25'
          memory: 512M
    # Disable inherited health check - indexer doesn't run HTTP server
    healthcheck:
      test: ["CMD-SHELL", "true"]
      interval: 60s

  # ---------------------------------------------------------------------------
  # PostgreSQL 16 - Primary metadata store
  # ---------------------------------------------------------------------------
  postgres:
    image: postgres:16-alpine
    container_name: chive-postgres
    restart: unless-stopped
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-chive}
      POSTGRES_USER: ${POSTGRES_USER:-chive}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:?POSTGRES_PASSWORD is required}
      # Performance tuning for small server
      POSTGRES_INITDB_ARGS: "--data-checksums"
    command:
      - "postgres"
      - "-c"
      - "shared_buffers=256MB"
      - "-c"
      - "effective_cache_size=768MB"
      - "-c"
      - "maintenance_work_mem=64MB"
      - "-c"
      - "checkpoint_completion_target=0.9"
      - "-c"
      - "wal_buffers=8MB"
      - "-c"
      - "default_statistics_target=100"
      - "-c"
      - "random_page_cost=1.1"
      - "-c"
      - "effective_io_concurrency=200"
      - "-c"
      - "work_mem=8MB"
      - "-c"
      - "huge_pages=off"
      - "-c"
      - "min_wal_size=1GB"
      - "-c"
      - "max_wal_size=4GB"
      - "-c"
      - "max_connections=100"
      - "-c"
      - "log_statement=none"
      - "-c"
      - "log_min_duration_statement=1000"
    volumes:
      - postgres_data:/var/lib/postgresql/data
    ports:
      # Bind to localhost only for SSH tunnel access (admin scripts)
      - "127.0.0.1:5432:5432"
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 1G
        reservations:
          cpus: '0.5'
          memory: 512M
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-chive} -d ${POSTGRES_DB:-chive}"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 30s

  # ---------------------------------------------------------------------------
  # Redis 7 - Caching, rate limiting, job queue
  # ---------------------------------------------------------------------------
  redis:
    image: redis:7-alpine
    container_name: chive-redis
    restart: unless-stopped
    command:
      - "redis-server"
      - "--maxmemory"
      - "256mb"
      - "--maxmemory-policy"
      - "allkeys-lru"
      - "--appendonly"
      - "yes"
      - "--appendfsync"
      - "everysec"
      - "--save"
      - "900 1"
      - "--save"
      - "300 10"
      - "--save"
      - "60 10000"
    volumes:
      - redis_data:/data
    ports:
      # Bind to localhost only for SSH tunnel access (admin scripts)
      - "127.0.0.1:6379:6379"
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 300M
        reservations:
          cpus: '0.1'
          memory: 128M
    healthcheck:
      test: ["CMD", "redis-cli", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 10s

  # ---------------------------------------------------------------------------
  # Elasticsearch 8 - Full-text search
  # ---------------------------------------------------------------------------
  # Memory allocation follows Elasticsearch best practices:
  # - Heap: 50% of container memory (max 32GB for compressed oops)
  # - Remaining: OS file cache for Lucene segments
  elasticsearch:
    image: docker.elastic.co/elasticsearch/elasticsearch:8.11.0
    container_name: chive-elasticsearch
    restart: unless-stopped
    environment:
      - discovery.type=single-node
      - xpack.security.enabled=false
      - xpack.security.enrollment.enabled=false
      # Heap: 1.5GB (50% of 3GB container)
      - "ES_JAVA_OPTS=-Xms1536m -Xmx1536m"
      - cluster.name=chive-cluster
      - bootstrap.memory_lock=true
      - cluster.routing.allocation.disk.threshold_enabled=true
      - cluster.routing.allocation.disk.watermark.low=85%
      - cluster.routing.allocation.disk.watermark.high=90%
    volumes:
      - elasticsearch_data:/usr/share/elasticsearch/data
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '2.0'
          memory: 3G
        reservations:
          cpus: '0.5'
          memory: 1536M
    ulimits:
      memlock:
        soft: -1
        hard: -1
      nofile:
        soft: 65536
        hard: 65536
    healthcheck:
      test: ["CMD-SHELL", "curl -sf http://localhost:9200/_cluster/health?wait_for_status=yellow&timeout=30s || exit 1"]
      interval: 30s
      timeout: 30s
      retries: 5
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Neo4j 5 - Knowledge graph
  # ---------------------------------------------------------------------------
  # Memory allocation follows Neo4j best practices:
  # - Heap: 50% of container memory for query processing
  # - Page cache: 25-30% for graph data caching
  # - Remaining: OS overhead, buffers, and transaction memory
  neo4j:
    image: neo4j:5-community
    container_name: chive-neo4j
    restart: unless-stopped
    environment:
      NEO4J_AUTH: ${NEO4J_USER:-neo4j}/${NEO4J_PASSWORD:?NEO4J_PASSWORD is required}
      NEO4J_PLUGINS: '["apoc"]'
      NEO4J_dbms_security_procedures_unrestricted: apoc.*
      NEO4J_dbms_security_procedures_allowlist: apoc.*
      # Memory configuration - sized for 2.5GB container
      # Heap: 1.25GB (50% of container) for query processing
      NEO4J_server_memory_heap_initial__size: 1g
      NEO4J_server_memory_heap_max__size: 1280m
      # Page cache: 768MB (~30%) for caching graph data
      NEO4J_server_memory_pagecache_size: 768m
      # Transaction memory pool
      NEO4J_dbms_memory_transaction_total_max: 384m
      # Query cache for repeated queries
      NEO4J_db_query__cache__size: 1000
    volumes:
      - neo4j_data:/data
      - neo4j_logs:/logs
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '1.5'
          memory: 2560M
        reservations:
          cpus: '0.5'
          memory: 1536M
    healthcheck:
      test: ["CMD-SHELL", "wget -qO- http://localhost:7474 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
      start_period: 60s

  # ---------------------------------------------------------------------------
  # Grafana Alloy - Unified Telemetry Collector
  # ---------------------------------------------------------------------------
  # Receives Faro telemetry (frontend) and OTLP (backend), routes to Tempo/Loki
  alloy:
    image: grafana/alloy:v1.5.1
    container_name: chive-alloy
    restart: unless-stopped
    command:
      - run
      - /etc/alloy/config.alloy
      - --server.http.listen-addr=0.0.0.0:12345
      - --stability.level=generally-available
    volumes:
      - ./observability/alloy-config.alloy:/etc/alloy/config.alloy:ro
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    depends_on:
      - tempo
      - loki
    healthcheck:
      # Alloy image doesn't include wget/curl, use bash TCP
      test: ["CMD-SHELL", "bash -c 'exec 3<>/dev/tcp/localhost/12345 && echo -e \"GET /-/ready HTTP/1.0\\r\\n\\r\\n\" >&3 && grep -q \"200 OK\" <&3'"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - observability
    labels:
      - "traefik.enable=true"
      # Faro collector endpoint (frontend telemetry)
      - "traefik.http.routers.faro.rule=Host(`faro.${DOMAIN}`)"
      - "traefik.http.routers.faro.entrypoints=websecure"
      - "traefik.http.routers.faro.tls.certresolver=letsencrypt"
      - "traefik.http.services.faro.loadbalancer.server.port=12347"

  # ---------------------------------------------------------------------------
  # Tempo - Distributed Tracing Backend
  # ---------------------------------------------------------------------------
  tempo:
    image: grafana/tempo:2.6.1
    container_name: chive-tempo
    restart: unless-stopped
    command:
      - -config.file=/etc/tempo/config.yaml
    volumes:
      - ./observability/tempo-config.yaml:/etc/tempo/config.yaml:ro
      - tempo_data:/var/tempo
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3200/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - observability

  # ---------------------------------------------------------------------------
  # Loki - Log Aggregation Backend
  # ---------------------------------------------------------------------------
  loki:
    image: grafana/loki:3.3.2
    container_name: chive-loki
    restart: unless-stopped
    command:
      - -config.file=/etc/loki/config.yaml
    volumes:
      - ./observability/loki-config.yaml:/etc/loki/config.yaml:ro
      - loki_data:/loki
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3100/ready"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - observability

  # ---------------------------------------------------------------------------
  # Prometheus - Metrics Backend
  # ---------------------------------------------------------------------------
  # Receives metrics from Tempo's metrics_generator for service graphs
  prometheus:
    image: prom/prometheus:v2.54.1
    container_name: chive-prometheus
    restart: unless-stopped
    command:
      - "--config.file=/etc/prometheus/prometheus.yml"
      - "--storage.tsdb.path=/prometheus"
      - "--storage.tsdb.retention.time=7d"
      - "--web.enable-remote-write-receiver"
      - "--web.console.libraries=/etc/prometheus/console_libraries"
      - "--web.console.templates=/etc/prometheus/consoles"
    volumes:
      - ./observability/prometheus.yml:/etc/prometheus/prometheus.yml:ro
      - prometheus_data:/prometheus
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 512M
        reservations:
          cpus: '0.1'
          memory: 256M
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:9090/-/healthy"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - observability

  # ---------------------------------------------------------------------------
  # Grafana - Visualization & Dashboards
  # ---------------------------------------------------------------------------
  grafana:
    image: grafana/grafana:11.4.0
    container_name: chive-grafana
    restart: unless-stopped
    environment:
      - GF_SECURITY_ADMIN_USER=${GRAFANA_ADMIN_USER:-admin}
      - GF_SECURITY_ADMIN_PASSWORD=${GRAFANA_ADMIN_PASSWORD:?GRAFANA_ADMIN_PASSWORD is required}
      - GF_USERS_ALLOW_SIGN_UP=false
      - GF_INSTALL_PLUGINS=grafana-lokiexplore-app
      - GF_FEATURE_TOGGLES_ENABLE=traceqlEditor tempoServiceGraph traceQLStreaming metricsSummary
    volumes:
      - ./observability/grafana/provisioning:/etc/grafana/provisioning:ro
      - grafana_data:/var/lib/grafana
    networks:
      - chive-network
    deploy:
      resources:
        limits:
          cpus: '0.5'
          memory: 256M
        reservations:
          cpus: '0.1'
          memory: 128M
    depends_on:
      - tempo
      - loki
      - prometheus
    healthcheck:
      test: ["CMD", "wget", "-qO-", "http://localhost:3000/api/health"]
      interval: 30s
      timeout: 10s
      retries: 3
      start_period: 30s
    profiles:
      - observability
    labels:
      - "traefik.enable=true"
      - "traefik.http.routers.grafana.rule=Host(`grafana.${DOMAIN}`)"
      - "traefik.http.routers.grafana.entrypoints=websecure"
      - "traefik.http.routers.grafana.tls.certresolver=letsencrypt"
      - "traefik.http.services.grafana.loadbalancer.server.port=3000"

# =============================================================================
# Volumes
# =============================================================================
volumes:
  traefik_certs:
    name: chive_traefik_certs
  governance_pds_data:
    name: chive_governance_pds_data
  postgres_data:
    name: chive_postgres_data
  redis_data:
    name: chive_redis_data
  elasticsearch_data:
    name: chive_elasticsearch_data
  neo4j_data:
    name: chive_neo4j_data
  neo4j_logs:
    name: chive_neo4j_logs
  tempo_data:
    name: chive_tempo_data
  loki_data:
    name: chive_loki_data
  prometheus_data:
    name: chive_prometheus_data
  grafana_data:
    name: chive_grafana_data

# =============================================================================
# Networks
# =============================================================================
networks:
  chive-network:
    name: chive-network
    driver: bridge
